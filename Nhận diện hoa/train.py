# =============================
# 1. K·∫æT N·ªêI GOOGLE DRIVE
# =============================
from google.colab import drive
drive.mount('/content/drive')

# =============================
# 2. T·∫¢I V√Ä SAO CH√âP DATASET
# =============================
import os
import shutil
import pathlib
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical

drive_dataset = "/content/drive/MyDrive/flower_photos/flower_photos"
# T·∫°o m·ªôt th∆∞ m·ª•c g·ªëc ƒë·ªÉ sao ch√©p
local_base_dir = "/content/dataset"

# Ki·ªÉm tra n·∫øu dataset ƒë√£ c√≥ tr√™n Drive th√¨ copy qua, n·∫øu kh√¥ng th√¨ t·∫£i v·ªÅ
if os.path.exists(drive_dataset):
    print("üìÅ Sao ch√©p dataset t·ª´ Google Drive v·ªÅ local...")
    if os.path.exists(local_base_dir):
        shutil.rmtree(local_base_dir)
    shutil.copytree(drive_dataset, local_base_dir)
else:
    print("üåê T·∫£i dataset t·ª´ TensorFlow...")
    dataset_url = "https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"
    data_dir_path = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)
    # L∆∞u v√†o c·∫£ Drive v√† local
    shutil.copytree(data_dir_path, drive_dataset, dirs_exist_ok=True)
    shutil.copytree(data_dir_path, local_base_dir, dirs_exist_ok=True)

# !!! D√íNG QUAN TR·ªåNG ƒê√É ƒê∆Ø·ª¢C S·ª¨A !!!
# Tr·ªè th·∫≥ng v√†o th∆∞ m·ª•c ch·ª©a c√°c l·ªõp hoa
local_dataset = local_base_dir

print("‚úÖ T·∫£i v√† sao ch√©p dataset th√†nh c√¥ng!")


# =============================
# 3. TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU
# =============================
print("\n‚è≥ B·∫Øt ƒë·∫ßu ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu...")
img_size = (64, 64)
X, y = [], []

# L·∫•y danh s√°ch t√™n c√°c l·ªõp (th∆∞ m·ª•c)
class_names = sorted([d for d in os.listdir(local_dataset) if os.path.isdir(os.path.join(local_dataset, d))])
print(f"C√°c l·ªõp ƒë∆∞·ª£c t√¨m th·∫•y: {class_names}")

# ƒê·ªçc t·ª´ng ·∫£nh, chuy·ªÉn th√†nh m·∫£ng v√† g√°n nh√£n
for label, folder_name in enumerate(class_names):
    folder_path = os.path.join(local_dataset, folder_name)
    for file in os.listdir(folder_path):
        img_path = os.path.join(folder_path, file)
        try:
            img = load_img(img_path, target_size=img_size)
            arr = img_to_array(img)
            X.append(arr)
            y.append(label)
        except Exception as e:
            # B·ªè qua c√°c file kh√¥ng ph·∫£i ·∫£nh
            # print(f"B·ªè qua file {img_path} v√¨ l·ªói: {e}") # B·ªè comment n·∫øu mu·ªën debug
            continue

# Chu·∫©n h√≥a d·ªØ li·ªáu ·∫£nh v·ªÅ kho·∫£ng [0, 1] v√† chuy·ªÉn nh√£n v·ªÅ d·∫°ng one-hot
X = np.array(X, dtype="float32") / 255.0
y = to_categorical(np.array(y), num_classes=len(class_names))

# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print(f"ƒê√£ t√¨m th·∫•y v√† x·ª≠ l√Ω {len(X)} ·∫£nh.")
print(f"K√≠ch th∆∞·ªõc t·∫≠p hu·∫•n luy·ªán (X_train): {X_train.shape}")
print(f"K√≠ch th∆∞·ªõc t·∫≠p ki·ªÉm th·ª≠ (X_test): {X_test.shape}")
print("‚úÖ Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ho√†n t·∫•t!")

# =============================
# 4. DATA AUGMENTATION (TƒÇNG C∆Ø·ªúNG D·ªÆ LI·ªÜU)
# =============================
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)
datagen.fit(X_train)
print("\n‚öôÔ∏è  ƒê√£ c·∫•u h√¨nh Data Augmentation.")

# =============================
# 5. X√ÇY D·ª∞NG M√î H√åNH ANN
# =============================
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Flatten
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

model = Sequential([
    Flatten(input_shape=(64, 64, 3)),
    Dense(512, activation='relu'),
    BatchNormalization(),
    Dropout(0.4),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.3),
    Dense(len(class_names), activation='softmax')
])

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

callbacks = [
    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1, min_lr=1e-6),
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
    ModelCheckpoint('/content/drive/MyDrive/final_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)
]
print("‚úÖ X√¢y d·ª±ng m√¥ h√¨nh th√†nh c√¥ng!")

# =============================
# 6. HU·∫§N LUY·ªÜN M√î H√åNH
# =============================
print("\nüöÄ B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...")
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    validation_data=(X_test, y_test),
    epochs=25,
    callbacks=callbacks
)

print("\nüéâ ƒê√£ hu·∫•n luy·ªán xong v√† l∆∞u m√¥ h√¨nh t·ªët nh·∫•t v√†o Google Drive: final_model.h5")